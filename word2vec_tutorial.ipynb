{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec, или о практике создания векторных моделей языка\n",
    "\n",
    "*Тут будет вступление.*\n",
    "- *о word2vec*\n",
    "- *o реализации word2vec в библиотеке gensim*\n",
    "\n",
    "Тьюториал состоит из трех частей:\n",
    "* в первой части мы научимся осуществлять предобработку текстовых файлов и тренировать векторную модель.\n",
    "* во второй части мы научимся загружать векторные модели и работать с ними, например, выполнять простые операции над векторами слов, такие как \"сложить вектора двух слов\" или \"вычислить коэффициент близости между двумя векторами слов\". \n",
    "* в третьей части мы рассмотрим более сложные операции над векторами, например, \"найти семантические аналоги\" или \"найти лишний вектор\".\n",
    "\n",
    "Мы рекомендуем использовать **Python3**, работоспособность тьюториала для **Python2** не гарантируется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Предобработка текстовых данных и тренировка модели\n",
    "\n",
    "*Сказать о том, что предобрабатывать данные не обязательно, можно просто разделить на предложения, но мы сделаем более глубокую предобработку для совместимости с моделями RusVectores. Ну и просто потому что полезно*\n",
    "\n",
    "### Обработка текста\n",
    "\n",
    "Предобработка текстов для тренировки моделей выглядит следующим образом:\n",
    "* сначала мы приведем все слова к начальной форме (лемматизируем) и удалим стоп-слова;\n",
    "* затем мы приведем все леммы к нижнему регистру;\n",
    "* для каждого слова добавим его частеречный тэг.\n",
    "\n",
    "Давайте попробуем воссоздать процесс предобработки текста на примере рассказа [О. Генри \"Русские соболя\"](https://rusvectores.org/static/henry_sobolya.txt). Для предобработки можно использовать различные тэггеры, мы сейчас будем использовать [*UDPipe*](https://ufal.mff.cuni.cz/udpipe), чтобы сразу получить частеречную разметку в виде Universal POS-tags. Сначала установим обертку *UDPipe* для Python:\n",
    "\n",
    "`pip install ufal.udpipe`\n",
    "\n",
    "*UDPipe* использует предобученные модели для лемматизации и тэггинга. Вы можете использовать [уже готовую модель](https://rusvectores.org/static/models/udpipe_syntagrus.model) или обучить свою. \n",
    "\n",
    "Чтобы загружать файлы, можно использовать реализацию wget в виде питоновской библиотеки:\n",
    "\n",
    "`pip install wget`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T13:41:17.556114Z",
     "start_time": "2021-07-02T13:41:10.672472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9680 sha256=11803e6beea9c5354e9e189b0c391e34adf1ce845ca81d2c0155fe27217fe3fe\n",
      "  Stored in directory: c:\\users\\arcry\\appdata\\local\\pip\\cache\\wheels\\a1\\b6\\7c\\0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Collecting ufal.udpipe\n",
      "  Downloading ufal.udpipe-1.2.0.3-cp37-cp37m-win_amd64.whl (691 kB)\n",
      "Installing collected packages: ufal.udpipe\n",
      "Successfully installed ufal.udpipe-1.2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install wget\n",
    "!pip install ufal.udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T13:41:34.960751Z",
     "start_time": "2021-07-02T13:41:24.433642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..............................................................................] 25649 / 25649"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import sys\n",
    "\n",
    "udpipe_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "text_url = 'https://rusvectores.org/static/henry_sobolya.txt'\n",
    "\n",
    "modelfile = wget.download(udpipe_url)\n",
    "textfile = wget.download(text_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приступим к собственно предобработке текста. Попробуем лемматизировать текст и добавить частеречные тэги при помощи этой функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T13:45:53.830896Z",
     "start_time": "2021-07-02T13:45:53.809921Z"
    }
   },
   "outputs": [],
   "source": [
    "def process(pipeline, text='Строка', keep_pos=True, keep_punct=False):\n",
    "    entities = {'PROPN'}\n",
    "    named = False\n",
    "    memory = []\n",
    "    mem_case = None\n",
    "    mem_number = None\n",
    "    tagged_propn = []\n",
    "\n",
    "    # обрабатываем текст, получаем результат в формате conllu:\n",
    "    processed = pipeline.process(text)\n",
    "\n",
    "    # пропускаем строки со служебной информацией:\n",
    "    content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
    "\n",
    "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
    "    tagged = [w.split('\\t') for w in content if w]\n",
    "\n",
    "    for t in tagged:\n",
    "        if len(t) != 10:\n",
    "            continue\n",
    "        (word_id, token, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
    "        if not lemma or not token:\n",
    "            continue\n",
    "        if pos in entities:\n",
    "            if '|' not in feats:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            morph = {el.split('=')[0]: el.split('=')[1] for el in feats.split('|')}\n",
    "            if 'Case' not in morph or 'Number' not in morph:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            if not named:\n",
    "                named = True\n",
    "                mem_case = morph['Case']\n",
    "                mem_number = morph['Number']\n",
    "            if morph['Case'] == mem_case and morph['Number'] == mem_number:\n",
    "                memory.append(lemma)\n",
    "                if 'SpacesAfter=\\\\n' in misc or 'SpacesAfter=\\s\\\\n' in misc:\n",
    "                    named = False\n",
    "                    past_lemma = '::'.join(memory)\n",
    "                    memory = []\n",
    "                    tagged_propn.append(past_lemma + '_PROPN ')\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "        else:\n",
    "            if not named:\n",
    "                if pos == 'NUM' and token.isdigit():  # Заменяем числа на xxxxx той же длины\n",
    "                    lemma = num_replace(token)\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "\n",
    "    if not keep_punct:\n",
    "        tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
    "    if not keep_pos:\n",
    "        tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
    "    return tagged_propn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эту функцию можно также изменить под конкретную задачу. Например, если частеречные тэги нам не нужны, в функции ниже выставим `keep_pos=False`. Если необходимо сохранить знаки пунктуации, можно выставить `keep_punct=True`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь загружаем модель *UDPipe*, читаем текстовый файл и обрабатываем его при помощи нашей функции. В файле должен содержаться необработанный текст (одно предложение на строку или один абзац на строку).\n",
    "Этот текст токенизируется, лемматизируется и размечается по частям речи с использованием UDPipe.\n",
    "На выход мы получаем последовательность разделенных пробелами лемм с частями речи (\"зеленый\\_NOUN трамвай\\_NOUN\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T13:46:02.219108Z",
     "start_time": "2021-07-02T13:46:02.201112Z"
    }
   },
   "outputs": [],
   "source": [
    "from ufal.udpipe import Model, Pipeline\n",
    "import os\n",
    "import re\n",
    "\n",
    "def tag_ud(text='Текст нужно передать функции в виде строки!', modelfile='udpipe_syntagrus.model'):\n",
    "    udpipe_model_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "    udpipe_filename = udpipe_model_url.split('/')[-1]\n",
    "\n",
    "    if not os.path.isfile(modelfile):\n",
    "        print('UDPipe model not found. Downloading...', file=sys.stderr)\n",
    "        wget.download(udpipe_model_url)\n",
    "\n",
    "    print('\\nLoading the model...', file=sys.stderr)\n",
    "    model = Model.load(modelfile)\n",
    "    process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "\n",
    "    print('Processing input...', file=sys.stderr)\n",
    "    lines = text.split('\\n')\n",
    "    tagged = []\n",
    "    for line in lines:\n",
    "        # line = unify_sym(line.strip()) # здесь могла бы быть ваша функция очистки текста\n",
    "        output = process(process_pipeline, text=line)\n",
    "        tagged_line = ' '.join(output)\n",
    "        tagged.append(tagged_line)\n",
    "    return '\\n'.join(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T13:47:19.415707Z",
     "start_time": "2021-07-02T13:47:14.299425Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the model...\n",
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "русский_PROPN  соболь_NOUN о.::генри_PROPN \n",
      "когда_SCONJ синий_ADJ как_SCONJ ночь_NOUN глаз_NOUN Молли_VERB Мак-Кивер_PROPN  класть_VERB малыш::Брэди_PROPN  на_ADP оба_NUM лопатка_NOUN он_PRON вынужденный_ADJ быть_AUX покидать_VERB ряд_NOUN банда_NOUN «Дымовый_ADJ труба»_NOUN таков_ADJ власть_NOUN нежный_ADJ укор_NOUN подружка_NOUN и_CCONJ она_PRON \n"
     ]
    }
   ],
   "source": [
    "text = open(textfile, 'r', encoding='utf-8').read()\n",
    "processed_text = tag_ud(text=text, modelfile=modelfile)\n",
    "print(processed_text[:350])\n",
    "with open('my_text.txt', 'w', encoding='utf-8') as out:\n",
    "    out.write(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша функция напечатает обработанный текст, который мы теперь можем также сохранить в файл. \n",
    "\n",
    "Итак, в ходе этой части тьюториала мы научились от \"сырого текста\" приходить к лемматизированному тексту с частеречными тэгами, который уже можно подавать на вход модели! Теперь теперь попробуем натренировать векторную модель.\n",
    "\n",
    "### Процесс тренировки модели \n",
    "\n",
    "Для работы с эмбеддингами слов существуют различные библиотеки: [gensim](https://radimrehurek.com/gensim/), [keras](https://keras.io/), [tensorflow](https://www.tensorflow.org/), [pytorch](https://pytorch.org/). Мы будем работать с библиотекой *gensim*, ведь в основе нашего сервера именно она и используется.\n",
    "\n",
    "\n",
    "***Gensim***  - изначально библиотека для тематического моделирования текстов. Однако помимо различных алгоритмов для *topic modeling* в ней реализованы на python и алгоритмы из тулкита *word2vec* (который в оригинале был написан на C++). Прежде всего, если *gensim* у вас на компьютере не установлен, нужно его установить:\n",
    "\n",
    "`pip install gensim`\n",
    "\n",
    "Gensim регулярно обновляется, так что не будет лишним удостовериться, что у вас установлена последняя версия, а при необходимости проапдейтить библиотеку:\n",
    "\n",
    "`pip install gensim --upgrade` \n",
    "\n",
    "или \n",
    "\n",
    "`pip install gensim -U`\n",
    "\n",
    "При подготовке этого тьюториала использовался *gensim* версии 3.7.0.\n",
    "\n",
    "Поскольку обучение и загрузка моделей могут занимать продолжительное время, иногда бывает полезно вести лог событий. Для этого используется стандартная питоновская библиотека `logging`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T13:56:57.048707Z",
     "start_time": "2021-07-02T13:56:55.856503Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\arcry\\anaconda3\\envs\\myenv\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход модели мы даем наш обработанный текстовый файл (либо любой другой текст, важно лишь, что каждое предложение должно быть на отдельной строчке)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T14:00:31.897158Z",
     "start_time": "2021-07-02T14:00:31.890198Z"
    }
   },
   "outputs": [],
   "source": [
    "f = 'C:/Users/Arcry/SkillBox/my_text.txt'\n",
    "data = gensim.models.word2vec.LineSentence(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T14:00:37.481371Z",
     "start_time": "2021-07-02T14:00:37.473375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.LineSentence at 0x19cd14efdc8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем модель. Параметры в скобочках:\n",
    "* data - данные, \n",
    "* size - размер вектора, \n",
    "* window - размер окна наблюдения,\n",
    "* min_count - мин. частотность слова в корпусе, которое мы берем,\n",
    "* sg - используемый алгоритм обучение (0 - CBOW, 1 - Skip-gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T14:00:42.963541Z",
     "start_time": "2021-07-02T14:00:42.881544Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-02 17:00:42,883 : INFO : collecting all words and their counts\n",
      "2021-07-02 17:00:42,884 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-07-02 17:00:42,887 : INFO : collected 981 word types from a corpus of 2085 raw words and 65 sentences\n",
      "2021-07-02 17:00:42,888 : INFO : Creating a fresh vocabulary\n",
      "2021-07-02 17:00:42,890 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 252 unique words (25.68807339449541%% of original 981, drops 729)', 'datetime': '2021-07-02T17:00:42.890557', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 26 2021, 13:06:18) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-07-02 17:00:42,891 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 1356 word corpus (65.03597122302158%% of original 2085, drops 729)', 'datetime': '2021-07-02T17:00:42.891543', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 26 2021, 13:06:18) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-07-02 17:00:42,893 : INFO : deleting the raw counts dictionary of 981 items\n",
      "2021-07-02 17:00:42,894 : INFO : sample=0.001 downsamples 89 most-common words\n",
      "2021-07-02 17:00:42,895 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 801.9147957961445 word corpus (59.1%% of prior 1356)', 'datetime': '2021-07-02T17:00:42.895542', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 26 2021, 13:06:18) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-07-02 17:00:42,898 : INFO : estimated required memory for 252 words and 500 dimensions: 1134000 bytes\n",
      "2021-07-02 17:00:42,899 : INFO : resetting layer weights\n",
      "2021-07-02 17:00:42,901 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-07-02T17:00:42.901542', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 26 2021, 13:06:18) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n",
      "2021-07-02 17:00:42,901 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 252 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=10', 'datetime': '2021-07-02T17:00:42.901542', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 26 2021, 13:06:18) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2021-07-02 17:00:42,906 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-02 17:00:42,907 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-02 17:00:42,909 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-02 17:00:42,910 : INFO : EPOCH - 1 : training on 2085 raw words (785 effective words) took 0.0s, 144730 effective words/s\n",
      "2021-07-02 17:00:42,915 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-02 17:00:42,916 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-02 17:00:42,918 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-02 17:00:42,919 : INFO : EPOCH - 2 : training on 2085 raw words (816 effective words) took 0.0s, 141642 effective words/s\n",
      "2021-07-02 17:00:42,925 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-02 17:00:42,926 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-02 17:00:42,929 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-02 17:00:42,929 : INFO : EPOCH - 3 : training on 2085 raw words (814 effective words) took 0.0s, 111655 effective words/s\n",
      "2021-07-02 17:00:42,934 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-02 17:00:42,935 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-02 17:00:42,938 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-02 17:00:42,939 : INFO : EPOCH - 4 : training on 2085 raw words (815 effective words) took 0.0s, 129517 effective words/s\n",
      "2021-07-02 17:00:42,943 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-02 17:00:42,944 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-02 17:00:42,946 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-02 17:00:42,946 : INFO : EPOCH - 5 : training on 2085 raw words (791 effective words) took 0.0s, 159081 effective words/s\n",
      "2021-07-02 17:00:42,947 : INFO : Word2Vec lifecycle event {'msg': 'training on 10425 raw words (4021 effective words) took 0.0s, 89248 effective words/s', 'datetime': '2021-07-02T17:00:42.947542', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 26 2021, 13:06:18) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2021-07-02 17:00:42,948 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=252, vector_size=500, alpha=0.025)', 'datetime': '2021-07-02T17:00:42.948542', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 26 2021, 13:06:18) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(data, vector_size=500, window=10, min_count=2, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно нормализовать вектора, тогда модель будет занимать меньше RAM. Однако после этого её нельзя дотренировывать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T14:00:50.808446Z",
     "start_time": "2021-07-02T14:00:50.802446Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\arcry\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2021-07-02 17:00:50,804 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим, сколько в модели слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T14:01:26.921610Z",
     "start_time": "2021-07-02T14:01:26.907580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n"
     ]
    }
   ],
   "source": [
    "print(len(model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сохраняем!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T14:01:34.332547Z",
     "start_time": "2021-07-02T14:01:34.314547Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-02 17:01:34,315 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'my.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-07-02T17:01:34.315584', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 26 2021, 13:06:18) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'saving'}\n",
      "2021-07-02 17:01:34,316 : INFO : not storing attribute cum_table\n",
      "2021-07-02 17:01:34,322 : INFO : saved my.model\n"
     ]
    }
   ],
   "source": [
    "model.save('my.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Работа с векторными моделями при помощи библиотеки Gensim\n",
    "\n",
    "Для своих индивидуальных нужд и экспериментов бывает полезно самому натренировать модель на нужных данных и с нужными параметрами. Но для каких-то общих целей модели уже есть как для русского языка, так и для английского.\n",
    "\n",
    "Модели для русского скачать можно здесь - https://rusvectores.org/ru/models/\n",
    "\n",
    "Существуют несколько форматов, в которых могут храниться модели. Во-первых, данные могут храниться в нативном формате *word2vec*, при этом модель может быть бинарной или не бинарной. Для загрузки модели в формате *word2vec* в классе `KeyedVectors` (в котором хранится большинство относящихся к дистрибутивным моделям функций) существует функция `load_word2vec_format`, а бинарность модели можно указать в аргументе `binary` (внизу будет пример). Помимо этого, модель можно хранить и в собственном формате *gensim*, для этого существует класс `Word2Vec` с функцией `load`. Поскольку модели бывают разных форматов, то для них написаны разные функции загрузки; бывает полезно учитывать это в своем скрипте. Наш код определяет тип модели по её расширению, но вообще файл с моделью может называться как угодно, жестких ограничений для расширения нет.\n",
    "\n",
    "Давайте скачаем новейшую модель для русского языка, созданную на основе [Национального корпуса русского языка (НКРЯ)](http://www.ruscorpora.ru/), и загрузим в её в память. Распаковывать скачанный архив для обычных моделей не нужно, так как его содержимое прочитается при помощи специальной инструкции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T14:03:47.487735Z",
     "start_time": "2021-07-02T14:02:01.217827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99% [..................................................................... ] 484237312 / 484452285"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-02 17:03:43,899 : INFO : loading projection weights from <zipfile.ZipExtFile name='model.bin' mode='r' compress_type=deflate>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 99% [..................................................................... ] 484245504 / 484452285\r",
      " 99% [..................................................................... ] 484253696 / 484452285\r",
      " 99% [..................................................................... ] 484261888 / 484452285\r",
      " 99% [..................................................................... ] 484270080 / 484452285\r",
      " 99% [..................................................................... ] 484278272 / 484452285\r",
      " 99% [..................................................................... ] 484286464 / 484452285\r",
      " 99% [..................................................................... ] 484294656 / 484452285\r",
      " 99% [..................................................................... ] 484302848 / 484452285\r",
      " 99% [..................................................................... ] 484311040 / 484452285\r",
      " 99% [..................................................................... ] 484319232 / 484452285\r",
      " 99% [..................................................................... ] 484327424 / 484452285\r",
      " 99% [..................................................................... ] 484335616 / 484452285\r",
      " 99% [..................................................................... ] 484343808 / 484452285\r",
      " 99% [..................................................................... ] 484352000 / 484452285\r",
      " 99% [..................................................................... ] 484360192 / 484452285\r",
      " 99% [..................................................................... ] 484368384 / 484452285\r",
      " 99% [..................................................................... ] 484376576 / 484452285\r",
      " 99% [..................................................................... ] 484384768 / 484452285\r",
      " 99% [..................................................................... ] 484392960 / 484452285\r",
      " 99% [..................................................................... ] 484401152 / 484452285\r",
      " 99% [..................................................................... ] 484409344 / 484452285\r",
      " 99% [..................................................................... ] 484417536 / 484452285\r",
      " 99% [..................................................................... ] 484425728 / 484452285\r",
      " 99% [..................................................................... ] 484433920 / 484452285\r",
      " 99% [..................................................................... ] 484442112 / 484452285\r",
      " 99% [..................................................................... ] 484450304 / 484452285\r",
      "100% [......................................................................] 484452285 / 484452285"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-02 17:03:47,471 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (189193, 300) matrix of type float32 from <zipfile.ZipExtFile [closed]>', 'binary': True, 'encoding': 'utf8', 'datetime': '2021-07-02T17:03:47.471737', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 26 2021, 13:06:18) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "model_url = 'http://vectors.nlpl.eu/repository/11/180.zip'\n",
    "m = wget.download(model_url)\n",
    "model_file = model_url.split('/')[-1]\n",
    "with zipfile.ZipFile(model_file, 'r') as archive:\n",
    "    stream = archive.open('model.bin')\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скажем, нам интересны такие слова (пример для русского языка):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T14:07:27.951067Z",
     "start_time": "2021-07-02T14:07:27.944038Z"
    }
   },
   "outputs": [],
   "source": [
    "words = ['день_NOUN', 'ночь_NOUN', 'человек_NOUN', 'семантика_NOUN', 'студент_NOUN', 'студент_ADJ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попросим у модели 10 ближайших соседей для каждого слова и коэффициент косинусной близости для каждого:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T14:07:29.507686Z",
     "start_time": "2021-07-02T14:07:29.242691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "день_NOUN\n",
      "неделя_NOUN 0.7375996112823486\n",
      "день_PROPN 0.706766664981842\n",
      "месяц_NOUN 0.7037326097488403\n",
      "час_NOUN 0.6643949747085571\n",
      "утро_NOUN 0.6526745557785034\n",
      "вечер_NOUN 0.6038411855697632\n",
      "сутки_NOUN 0.5923080444335938\n",
      "воскресенье_NOUN 0.5842781066894531\n",
      "полдень_NOUN 0.5743687748908997\n",
      "суббота_NOUN 0.534594714641571\n",
      "\n",
      "\n",
      "ночь_NOUN\n",
      "ночь_PROPN 0.8310785293579102\n",
      "вечер_NOUN 0.718367874622345\n",
      "рассвет_NOUN 0.6965948343276978\n",
      "ночи_NOUN 0.6920219659805298\n",
      "полночь_NOUN 0.670497715473175\n",
      "ночь_VERB 0.6615265011787415\n",
      "утро_NOUN 0.6263935565948486\n",
      "ночной_ADJ 0.6024709343910217\n",
      "полдень_NOUN 0.5835085511207581\n",
      "сумерки_NOUN 0.5671443939208984\n",
      "\n",
      "\n",
      "человек_NOUN\n",
      "человек_PROPN 0.7850059866905212\n",
      "человеческий_ADJ 0.5915265679359436\n",
      "существо_NOUN 0.5736929774284363\n",
      "народ_NOUN 0.5354466438293457\n",
      "личность_NOUN 0.5296981930732727\n",
      "человечество_NOUN 0.5282931327819824\n",
      "человкъ_PROPN 0.5047001242637634\n",
      "индивидуум_NOUN 0.5000404119491577\n",
      "нравственный_ADJ 0.4972919821739197\n",
      "потому_ADV 0.4929361939430237\n",
      "\n",
      "\n",
      "семантика_NOUN\n",
      "семантический_ADJ 0.801933228969574\n",
      "синтаксический_ADJ 0.7569340467453003\n",
      "модальный_ADJ 0.7296057343482971\n",
      "семантически_ADV 0.7209396958351135\n",
      "смысловой_ADJ 0.7159028053283691\n",
      "референция_NOUN 0.7135108709335327\n",
      "ноэтический_ADJ 0.7080267071723938\n",
      "языковой_ADJ 0.7067198753356934\n",
      "лингвистический_ADJ 0.692865788936615\n",
      "предикат_NOUN 0.6877546906471252\n",
      "\n",
      "\n",
      "студент_NOUN\n",
      "преподаватель_NOUN 0.6743764281272888\n",
      "студенческий_ADJ 0.6486334204673767\n",
      "университетский_ADJ 0.6442700028419495\n",
      "заочник_NOUN 0.6423173546791077\n",
      "первокурсник_NOUN 0.640970766544342\n",
      "курсистка_NOUN 0.6364570260047913\n",
      "дипломник_NOUN 0.6341054439544678\n",
      "аспирант_NOUN 0.6337910890579224\n",
      "университет_NOUN 0.6302101016044617\n",
      "студентка_NOUN 0.6299037337303162\n",
      "\n",
      "\n",
      "студент_ADJ is not present in the model\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    # есть ли слово в модели? Может быть, и нет\n",
    "    if word in model:\n",
    "        print(word)\n",
    "        # выдаем 10 ближайших соседей слова:\n",
    "        for i in model.most_similar(positive=[word], topn=10):\n",
    "            # слово + коэффициент косинусной близости\n",
    "            print(i[0], i[1])\n",
    "        print('\\n')\n",
    "    else:\n",
    "        # Увы!\n",
    "        print(word + ' is not present in the model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим косинусную близость пары слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T14:07:36.699013Z",
     "start_time": "2021-07-02T14:07:36.690986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22025342\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('человек_NOUN', 'обезьяна_NOUN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Более сложные операции над векторами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найди лишнее!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T14:07:42.267835Z",
     "start_time": "2021-07-02T14:07:42.251837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "картофель_NOUN\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match('яблоко_NOUN груша_NOUN виноград_NOUN банан_NOUN лимон_NOUN картофель_NOUN'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реши пропорцию!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T14:07:44.547656Z",
     "start_time": "2021-07-02T14:07:44.521657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "гамбургер_NOUN\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['пицца_NOUN', 'россия_NOUN'], negative=['италия_NOUN'])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Заключение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
