{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec, или о практике создания векторных моделей языка\n",
    "\n",
    "*Тут будет вступление.*\n",
    "- *о word2vec*\n",
    "- *o реализации word2vec в библиотеке gensim*\n",
    "\n",
    "Тьюториал состоит из трех частей:\n",
    "* в первой части мы научимся осуществлять предобработку текстовых файлов и тренировать векторную модель.\n",
    "* во второй части мы научимся загружать векторные модели и работать с ними, например, выполнять простые операции над векторами слов, такие как \"сложить вектора двух слов\" или \"вычислить коэффициент близости между двумя векторами слов\". \n",
    "* в третьей части мы рассмотрим более сложные операции над векторами, например, \"найти семантические аналоги\" или \"найти лишний вектор\".\n",
    "\n",
    "Мы рекомендуем использовать **Python3**, работоспособность тьюториала для **Python2** не гарантируется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Предобработка текстовых данных и тренировка модели\n",
    "\n",
    "*Сказать о том, что предобрабатывать данные не обязательно, можно просто разделить на предложения, но мы сделаем более глубокую предобработку для совместимости с моделями RusVectores. Ну и просто потому что полезно*\n",
    "\n",
    "### Обработка текста\n",
    "\n",
    "Предобработка текстов для тренировки моделей выглядит следующим образом:\n",
    "* сначала мы приведем все слова к начальной форме (лемматизируем) и удалим стоп-слова;\n",
    "* затем мы приведем все леммы к нижнему регистру;\n",
    "* для каждого слова добавим его частеречный тэг.\n",
    "\n",
    "Давайте попробуем воссоздать процесс предобработки текста на примере рассказа [О. Генри \"Русские соболя\"](https://rusvectores.org/static/henry_sobolya.txt). Для предобработки можно использовать различные тэггеры, мы сейчас будем использовать [*UDPipe*](https://ufal.mff.cuni.cz/udpipe), чтобы сразу получить частеречную разметку в виде Universal POS-tags. Сначала установим обертку *UDPipe* для Python:\n",
    "\n",
    "`pip install ufal.udpipe`\n",
    "\n",
    "*UDPipe* использует предобученные модели для лемматизации и тэггинга. Вы можете использовать [уже готовую модель](https://rusvectores.org/static/models/udpipe_syntagrus.model) или обучить свою. \n",
    "\n",
    "Чтобы загружать файлы, можно использовать реализацию wget в виде питоновской библиотеки:\n",
    "\n",
    "`pip install wget`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import sys\n",
    "\n",
    "udpipe_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "text_url = 'https://rusvectores.org/static/henry_sobolya.txt'\n",
    "\n",
    "modelfile = wget.download(udpipe_url)\n",
    "textfile = wget.download(text_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приступим к собственно предобработке текста. Попробуем лемматизировать текст и добавить частеречные тэги при помощи этой функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(pipeline, text='Строка', keep_pos=True, keep_punct=False):\n",
    "    entities = {'PROPN'}\n",
    "    named = False\n",
    "    memory = []\n",
    "    mem_case = None\n",
    "    mem_number = None\n",
    "    tagged_propn = []\n",
    "\n",
    "    # обрабатываем текст, получаем результат в формате conllu:\n",
    "    processed = pipeline.process(text)\n",
    "\n",
    "    # пропускаем строки со служебной информацией:\n",
    "    content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
    "\n",
    "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
    "    tagged = [w.split('\\t') for w in content if w]\n",
    "\n",
    "    for t in tagged:\n",
    "        if len(t) != 10:\n",
    "            continue\n",
    "        (word_id, token, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
    "        if not lemma or not token:\n",
    "            continue\n",
    "        if pos in entities:\n",
    "            if '|' not in feats:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            morph = {el.split('=')[0]: el.split('=')[1] for el in feats.split('|')}\n",
    "            if 'Case' not in morph or 'Number' not in morph:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            if not named:\n",
    "                named = True\n",
    "                mem_case = morph['Case']\n",
    "                mem_number = morph['Number']\n",
    "            if morph['Case'] == mem_case and morph['Number'] == mem_number:\n",
    "                memory.append(lemma)\n",
    "                if 'SpacesAfter=\\\\n' in misc or 'SpacesAfter=\\s\\\\n' in misc:\n",
    "                    named = False\n",
    "                    past_lemma = '::'.join(memory)\n",
    "                    memory = []\n",
    "                    tagged_propn.append(past_lemma + '_PROPN ')\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "        else:\n",
    "            if not named:\n",
    "                if pos == 'NUM' and token.isdigit():  # Заменяем числа на xxxxx той же длины\n",
    "                    lemma = num_replace(token)\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "\n",
    "    if not keep_punct:\n",
    "        tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
    "    if not keep_pos:\n",
    "        tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
    "    return tagged_propn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эту функцию можно также изменить под конкретную задачу. Например, если частеречные тэги нам не нужны, в функции ниже выставим `keep_pos=False`. Если необходимо сохранить знаки пунктуации, можно выставить `keep_punct=True`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь загружаем модель *UDPipe*, читаем текстовый файл и обрабатываем его при помощи нашей функции. В файле должен содержаться необработанный текст (одно предложение на строку или один абзац на строку).\n",
    "Этот текст токенизируется, лемматизируется и размечается по частям речи с использованием UDPipe.\n",
    "На выход мы получаем последовательность разделенных пробелами лемм с частями речи (\"зеленый\\_NOUN трамвай\\_NOUN\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ufal.udpipe import Model, Pipeline\n",
    "import os\n",
    "import re\n",
    "\n",
    "def tag_ud(text='Текст нужно передать функции в виде строки!', modelfile='udpipe_syntagrus.model'):\n",
    "    udpipe_model_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "    udpipe_filename = udpipe_model_url.split('/')[-1]\n",
    "\n",
    "    if not os.path.isfile(modelfile):\n",
    "        print('UDPipe model not found. Downloading...', file=sys.stderr)\n",
    "        wget.download(udpipe_model_url)\n",
    "\n",
    "    print('\\nLoading the model...', file=sys.stderr)\n",
    "    model = Model.load(modelfile)\n",
    "    process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "\n",
    "    print('Processing input...', file=sys.stderr)\n",
    "    lines = text.split('\\n')\n",
    "    tagged = []\n",
    "    for line in lines:\n",
    "        # line = unify_sym(line.strip()) # здесь могла бы быть ваша функция очистки текста\n",
    "        output = process(process_pipeline, text=line)\n",
    "        tagged_line = ' '.join(output)\n",
    "        tagged.append(tagged_line)\n",
    "    return '\\n'.join(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the model...\n",
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "русский_PROPN  соболь_NOUN о.::генри_PROPN \n",
      "когда_SCONJ синий_ADJ как_SCONJ ночь_NOUN глаз_NOUN Молли_VERB Мак-Кивер_PROPN  класть_VERB малыш::Брэди_PROPN  на_ADP оба_NUM лопатка_NOUN он_PRON вынужденный_ADJ быть_AUX покидать_VERB ряд_NOUN банда_NOUN «Дымовый_ADJ труба»_NOUN таков_ADJ власть_NOUN нежный_ADJ укор_NOUN подружка_NOUN и_CCONJ она_PRON \n"
     ]
    }
   ],
   "source": [
    "text = open(textfile, 'r', encoding='utf-8').read()\n",
    "processed_text = tag_ud(text=text, modelfile=modelfile)\n",
    "print(processed_text[:350])\n",
    "with open('my_text.txt', 'w', encoding='utf-8') as out:\n",
    "    out.write(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша функция напечатает обработанный текст, который мы теперь можем также сохранить в файл. \n",
    "\n",
    "Итак, в ходе этой части тьюториала мы научились от \"сырого текста\" приходить к лемматизированному тексту с частеречными тэгами, который уже можно подавать на вход модели! Теперь теперь попробуем натренировать векторную модель.\n",
    "\n",
    "### Процесс тренировки модели \n",
    "\n",
    "Для работы с эмбеддингами слов существуют различные библиотеки: [gensim](https://radimrehurek.com/gensim/), [keras](https://keras.io/), [tensorflow](https://www.tensorflow.org/), [pytorch](https://pytorch.org/). Мы будем работать с библиотекой *gensim*, ведь в основе нашего сервера именно она и используется.\n",
    "\n",
    "\n",
    "***Gensim***  - изначально библиотека для тематического моделирования текстов. Однако помимо различных алгоритмов для *topic modeling* в ней реализованы на python и алгоритмы из тулкита *word2vec* (который в оригинале был написан на C++). Прежде всего, если *gensim* у вас на компьютере не установлен, нужно его установить:\n",
    "\n",
    "`pip install gensim`\n",
    "\n",
    "Gensim регулярно обновляется, так что не будет лишним удостовериться, что у вас установлена последняя версия, а при необходимости проапдейтить библиотеку:\n",
    "\n",
    "`pip install gensim --upgrade` \n",
    "\n",
    "или \n",
    "\n",
    "`pip install gensim -U`\n",
    "\n",
    "При подготовке этого тьюториала использовался *gensim* версии 3.7.0.\n",
    "\n",
    "Поскольку обучение и загрузка моделей могут занимать продолжительное время, иногда бывает полезно вести лог событий. Для этого используется стандартная питоновская библиотека `logging`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход модели мы даем наш обработанный текстовый файл (либо любой другой текст, важно лишь, что каждое предложение должно быть на отдельной строчке)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'my_text.txt'\n",
    "data = gensim.models.word2vec.LineSentence(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем модель. Параметры в скобочках:\n",
    "* data - данные, \n",
    "* size - размер вектора, \n",
    "* window - размер окна наблюдения,\n",
    "* min_count - мин. частотность слова в корпусе, которое мы берем,\n",
    "* sg - используемый алгоритм обучение (0 - CBOW, 1 - Skip-gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-30 01:09:06,925 : INFO : collecting all words and their counts\n",
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-10-30 01:09:07,051 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-10-30 01:09:07,058 : INFO : collected 981 word types from a corpus of 2085 raw words and 65 sentences\n",
      "2019-10-30 01:09:07,060 : INFO : Loading a fresh vocabulary\n",
      "2019-10-30 01:09:07,065 : INFO : effective_min_count=2 retains 252 unique words (25% of original 981, drops 729)\n",
      "2019-10-30 01:09:07,067 : INFO : effective_min_count=2 leaves 1356 word corpus (65% of original 2085, drops 729)\n",
      "2019-10-30 01:09:07,074 : INFO : deleting the raw counts dictionary of 981 items\n",
      "2019-10-30 01:09:07,077 : INFO : sample=0.001 downsamples 89 most-common words\n",
      "2019-10-30 01:09:07,081 : INFO : downsampling leaves estimated 801 word corpus (59.1% of prior 1356)\n",
      "2019-10-30 01:09:07,130 : INFO : estimated required memory for 252 words and 500 dimensions: 1134000 bytes\n",
      "2019-10-30 01:09:07,136 : INFO : resetting layer weights\n",
      "2019-10-30 01:09:07,280 : INFO : training model with 3 workers on 252 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-10-30 01:09:07,292 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-30 01:09:07,293 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-30 01:09:07,300 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-30 01:09:07,301 : INFO : EPOCH - 1 : training on 2085 raw words (785 effective words) took 0.0s, 57367 effective words/s\n",
      "2019-10-30 01:09:07,326 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-30 01:09:07,328 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-30 01:09:07,331 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-30 01:09:07,333 : INFO : EPOCH - 2 : training on 2085 raw words (816 effective words) took 0.0s, 34336 effective words/s\n",
      "2019-10-30 01:09:07,348 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-30 01:09:07,357 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-30 01:09:07,361 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-30 01:09:07,363 : INFO : EPOCH - 3 : training on 2085 raw words (814 effective words) took 0.0s, 41298 effective words/s\n",
      "2019-10-30 01:09:07,374 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-30 01:09:07,378 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-30 01:09:07,384 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-30 01:09:07,385 : INFO : EPOCH - 4 : training on 2085 raw words (815 effective words) took 0.0s, 50628 effective words/s\n",
      "2019-10-30 01:09:07,405 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-30 01:09:07,410 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-30 01:09:07,415 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-30 01:09:07,417 : INFO : EPOCH - 5 : training on 2085 raw words (791 effective words) took 0.0s, 48407 effective words/s\n",
      "2019-10-30 01:09:07,419 : INFO : training on a 10425 raw words (4021 effective words) took 0.1s, 29464 effective words/s\n",
      "2019-10-30 01:09:07,420 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(data, size=500, window=10, min_count=2, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно нормализовать вектора, тогда модель будет занимать меньше RAM. Однако после этого её нельзя дотренировывать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-30 01:09:13,896 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим, сколько в модели слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n"
     ]
    }
   ],
   "source": [
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сохраняем!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-30 01:09:17,580 : INFO : saving Word2Vec object under my.model, separately None\n",
      "2019-10-30 01:09:17,584 : INFO : not storing attribute vectors_norm\n",
      "2019-10-30 01:09:17,587 : INFO : not storing attribute cum_table\n",
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-10-30 01:09:17,721 : INFO : saved my.model\n"
     ]
    }
   ],
   "source": [
    "model.save('my.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Работа с векторными моделями при помощи библиотеки Gensim\n",
    "\n",
    "Для своих индивидуальных нужд и экспериментов бывает полезно самому натренировать модель на нужных данных и с нужными параметрами. Но для каких-то общих целей модели уже есть как для русского языка, так и для английского.\n",
    "\n",
    "Модели для русского скачать можно здесь - https://rusvectores.org/ru/models/\n",
    "\n",
    "Существуют несколько форматов, в которых могут храниться модели. Во-первых, данные могут храниться в нативном формате *word2vec*, при этом модель может быть бинарной или не бинарной. Для загрузки модели в формате *word2vec* в классе `KeyedVectors` (в котором хранится большинство относящихся к дистрибутивным моделям функций) существует функция `load_word2vec_format`, а бинарность модели можно указать в аргументе `binary` (внизу будет пример). Помимо этого, модель можно хранить и в собственном формате *gensim*, для этого существует класс `Word2Vec` с функцией `load`. Поскольку модели бывают разных форматов, то для них написаны разные функции загрузки; бывает полезно учитывать это в своем скрипте. Наш код определяет тип модели по её расширению, но вообще файл с моделью может называться как угодно, жестких ограничений для расширения нет.\n",
    "\n",
    "Давайте скачаем новейшую модель для русского языка, созданную на основе [Национального корпуса русского языка (НКРЯ)](http://www.ruscorpora.ru/), и загрузим в её в память. Распаковывать скачанный архив для обычных моделей не нужно, так как его содержимое прочитается при помощи специальной инструкции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-30 01:12:15,586 : INFO : loading projection weights from <zipfile.ZipExtFile name='model.bin' mode='r' compress_type=deflate>\n",
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-10-30 01:12:59,029 : INFO : loaded (189193, 300) matrix from <zipfile.ZipExtFile [closed]>\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "model_url = 'http://vectors.nlpl.eu/repository/11/180.zip'\n",
    "m = wget.download(model_url)\n",
    "model_file = model_url.split('/')[-1]\n",
    "with zipfile.ZipFile(model_file, 'r') as archive:\n",
    "    stream = archive.open('model.bin')\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скажем, нам интересны такие слова (пример для русского языка):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['день_NOUN', 'ночь_NOUN', 'человек_NOUN', 'семантика_NOUN', 'студент_NOUN', 'студент_ADJ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попросим у модели 10 ближайших соседей для каждого слова и коэффициент косинусной близости для каждого:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-30 01:15:59,716 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "день_NOUN\n",
      "неделя_NOUN 0.7375996112823486\n",
      "день_PROPN 0.7067666053771973\n",
      "месяц_NOUN 0.7037326097488403\n",
      "час_NOUN 0.6643950343132019\n",
      "утро_NOUN 0.6526744365692139\n",
      "вечер_NOUN 0.6038411855697632\n",
      "сутки_NOUN 0.5923081040382385\n",
      "воскресенье_NOUN 0.5842781066894531\n",
      "полдень_NOUN 0.5743687748908997\n",
      "суббота_NOUN 0.5345946550369263\n",
      "\n",
      "\n",
      "ночь_NOUN\n",
      "ночь_PROPN 0.8310786485671997\n",
      "вечер_NOUN 0.7183679342269897\n",
      "рассвет_NOUN 0.696594774723053\n",
      "ночи_NOUN 0.692021906375885\n",
      "полночь_NOUN 0.6704976558685303\n",
      "ночь_VERB 0.6615264415740967\n",
      "утро_NOUN 0.6263935565948486\n",
      "ночной_ADJ 0.6024709939956665\n",
      "полдень_NOUN 0.5835086107254028\n",
      "сумерки_NOUN 0.5671443343162537\n",
      "\n",
      "\n",
      "человек_NOUN\n",
      "человек_PROPN 0.7850059270858765\n",
      "человеческий_ADJ 0.5915265679359436\n",
      "существо_NOUN 0.573693037033081\n",
      "народ_NOUN 0.5354466438293457\n",
      "личность_NOUN 0.5296981334686279\n",
      "человечество_NOUN 0.5282931327819824\n",
      "человкъ_PROPN 0.5047001838684082\n",
      "индивидуум_NOUN 0.5000404119491577\n",
      "нравственный_ADJ 0.4972919821739197\n",
      "потому_ADV 0.49293622374534607\n",
      "\n",
      "\n",
      "семантика_NOUN\n",
      "семантический_ADJ 0.8019332885742188\n",
      "синтаксический_ADJ 0.7569340467453003\n",
      "модальный_ADJ 0.7296056747436523\n",
      "семантически_ADV 0.7209396958351135\n",
      "смысловой_ADJ 0.7159026861190796\n",
      "референция_NOUN 0.7135108709335327\n",
      "ноэтический_ADJ 0.7080267071723938\n",
      "языковой_ADJ 0.7067198753356934\n",
      "лингвистический_ADJ 0.6928658485412598\n",
      "предикат_NOUN 0.68775475025177\n",
      "\n",
      "\n",
      "студент_NOUN\n",
      "преподаватель_NOUN 0.6743764281272888\n",
      "студенческий_ADJ 0.6486333608627319\n",
      "университетский_ADJ 0.6442699432373047\n",
      "заочник_NOUN 0.6423174142837524\n",
      "первокурсник_NOUN 0.6409708261489868\n",
      "курсистка_NOUN 0.636457085609436\n",
      "дипломник_NOUN 0.6341054439544678\n",
      "аспирант_NOUN 0.6337910890579224\n",
      "университет_NOUN 0.6302101612091064\n",
      "студентка_NOUN 0.6299037337303162\n",
      "\n",
      "\n",
      "студент_ADJ is not present in the model\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    # есть ли слово в модели? Может быть, и нет\n",
    "    if word in model:\n",
    "        print(word)\n",
    "        # выдаем 10 ближайших соседей слова:\n",
    "        for i in model.most_similar(positive=[word], topn=10):\n",
    "            # слово + коэффициент косинусной близости\n",
    "            print(i[0], i[1])\n",
    "        print('\\n')\n",
    "    else:\n",
    "        # Увы!\n",
    "        print(word + ' is not present in the model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим косинусную близость пары слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22025344\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('человек_NOUN', 'обезьяна_NOUN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Более сложные операции над векторами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найди лишнее!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "картофель_NOUN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:857: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match('яблоко_NOUN груша_NOUN виноград_NOUN банан_NOUN лимон_NOUN картофель_NOUN'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реши пропорцию!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "гамбургер_NOUN\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['пицца_NOUN', 'россия_NOUN'], negative=['италия_NOUN'])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Заключение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
